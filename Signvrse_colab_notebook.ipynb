{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCXAoxXYTyqn",
        "outputId": "c6b6df82-4a78-492e-cf18-770629554276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2025.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.8)\n",
            "Requirement already satisfied: mako in /usr/lib/python3/dist-packages (from pycuda) (1.1.3)\n",
            "Collecting siphash24>=1.6 (from pytools>=2011.2->pycuda)\n",
            "  Downloading siphash24-1.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.13.2)\n",
            "Downloading pytools-2025.1.6-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading siphash24-1.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2025.1-cp311-cp311-linux_x86_64.whl size=660424 sha256=82327545ac970278b7b6816699488cdef40fb045be19ee6260910ea841dd086d\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/7e/6c/d2d1451ea6424cdc3d67b36c16fa7111eafdf2034bc3405666\n",
            "Successfully built pycuda\n",
            "Installing collected packages: siphash24, pytools, pycuda\n",
            "Successfully installed pycuda-2025.1 pytools-2025.1.6 siphash24-1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install pycuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG-Czc1BUNhB",
        "outputId": "84240492-34c4-4598-97cb-c50a0a3bd104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Kernels Compiled Successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-469817ab6dfe>:93: UserWarning: The CUDA compiler succeeded, but said the following:\n",
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function 'rgb_to_grayscale_reference' for 'sm_75'\n",
            "ptxas info    : Function properties for rgb_to_grayscale_reference\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 12 registers, 376 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function 'rgb_to_grayscale_shared_mem' for 'sm_75'\n",
            "ptxas info    : Function properties for rgb_to_grayscale_shared_mem\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 14 registers, 3072 bytes smem, 376 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function 'rgb_to_grayscale_float3' for 'sm_75'\n",
            "ptxas info    : Function properties for rgb_to_grayscale_float3\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 12 registers, 376 bytes cmem[0]\n",
            "\n",
            "  module = SourceModule(cuda_source_code, options=['-Xptxas=-v']) # Example for verbose PTXAS output\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pycuda.autoinit # Initializes CUDA context\n",
        "import pycuda.driver as cuda\n",
        "from pycuda.compiler import SourceModule\n",
        "import time\n",
        "\n",
        "# Embed the CUDA kernel code as a Python string\n",
        "cuda_source_code = \"\"\"\n",
        "// rgb_to_grayscale_kernel.cu\n",
        "#include <cuda_runtime.h> // Includes vector_types.h (for float3) and math.h (for fmaf)\n",
        "\n",
        "// Optimized kernel using float3 for memory access\n",
        "__global__ void rgb_to_grayscale_float3(const float *__restrict__ input_raw,\n",
        "                                      float *__restrict__ output,\n",
        "                                      int width, int height) {\n",
        "    // Calculate global thread coordinates\n",
        "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    // Boundary check\n",
        "    if (x < width && y < height) {\n",
        "        int pixel_idx = y * width + x; // 1D index for the current pixel\n",
        "\n",
        "        const float3* input_float3_ptr = (const float3*)input_raw;\n",
        "\n",
        "        float3 rgb_val = input_float3_ptr[pixel_idx];\n",
        "\n",
        "        float gray_val = 0.2989f * rgb_val.x; // R component\n",
        "        gray_val = fmaf(0.5870f, rgb_val.y, gray_val); // Add G component term\n",
        "        gray_val = fmaf(0.1140f, rgb_val.z, gray_val); // Add B component term\n",
        "\n",
        "        output[pixel_idx] = gray_val;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// Bonus: Optimized kernel using shared memory.\n",
        "#define TILE_DIM_X 16 // Must match blockDim.x used in host code for this kernel\n",
        "#define TILE_DIM_Y 16 // Must match blockDim.y used in host code for this kernel\n",
        "\n",
        "__global__ void rgb_to_grayscale_shared_mem(const float *__restrict__ input_raw,\n",
        "                                          float *__restrict__ output,\n",
        "                                          int width, int height) {\n",
        "    __shared__ float3 tile_s[TILE_DIM_Y][TILE_DIM_X];\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    int global_x = blockIdx.x * TILE_DIM_X + tx;\n",
        "    int global_y = blockIdx.y * TILE_DIM_Y + ty;\n",
        "\n",
        "    const float3* input_float3_ptr = (const float3*)input_raw;\n",
        "\n",
        "    if (global_x < width && global_y < height) {\n",
        "        tile_s[ty][tx] = input_float3_ptr[global_y * width + global_x];\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (global_x < width && global_y < height) {\n",
        "        float3 rgb_val = tile_s[ty][tx];\n",
        "\n",
        "        float gray_val = 0.2989f * rgb_val.x;\n",
        "        gray_val = fmaf(0.5870f, rgb_val.y, gray_val);\n",
        "        gray_val = fmaf(0.1140f, rgb_val.z, gray_val);\n",
        "\n",
        "        output[global_y * width + global_x] = gray_val;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Reference kernel (similar to problem statement)\n",
        "__global__ void rgb_to_grayscale_reference(const float *__restrict__ input,\n",
        "                                         float *__restrict__ output,\n",
        "                                         int width, int height) {\n",
        "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (x < width && y < height) {\n",
        "        int idx = y * width + x;\n",
        "        int rgb_idx = idx * 3;\n",
        "\n",
        "        float r = input[rgb_idx + 0];\n",
        "        float g = input[rgb_idx + 1];\n",
        "        float b = input[rgb_idx + 2];\n",
        "\n",
        "        output[idx] = 0.2989f * r + 0.5870f * g + 0.1140f * b;\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Compile the CUDA source code\n",
        "# Adding -Xptxas -v can give insights into resource usage (registers, shared memory)\n",
        "module = SourceModule(cuda_source_code, options=['-Xptxas=-v']) #for verbose PTXAS output\n",
        "\n",
        "# Get kernel functions from the compiled module\n",
        "kernel_ref = module.get_function(\"rgb_to_grayscale_reference\")\n",
        "kernel_float3 = module.get_function(\"rgb_to_grayscale_float3\")\n",
        "kernel_shared_mem = module.get_function(\"rgb_to_grayscale_shared_mem\")\n",
        "\n",
        "print(\"CUDA Kernels Compiled Successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xGrksf9UOpi",
        "outputId": "caea849c-6a48-45c2-a1e5-7a7a8cdee82b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================\n",
            "Processing Image Size: 512x512\n",
            "======================================================\n",
            "\n",
            "--- Testing Reference Kernel for 512x512 image ---\n",
            "Execution time: 0.0185 ms (averaged over 100 runs)\n",
            "Correctness: PASS\n",
            "\n",
            "--- Testing Optimized Kernel (float3) for 512x512 image ---\n",
            "Execution time: 0.0186 ms (averaged over 100 runs)\n",
            "Correctness: PASS\n",
            "\n",
            "--- Testing Optimized Kernel (Shared Memory) for 512x512 image ---\n",
            "Execution time: 0.0208 ms (averaged over 100 runs)\n",
            "Correctness: PASS\n",
            "\n",
            "======================================================\n",
            "Processing Image Size: 1024x1024\n",
            "======================================================\n",
            "\n",
            "--- Testing Reference Kernel for 1024x1024 image ---\n",
            "Execution time: 0.0694 ms (averaged over 100 runs)\n",
            "Correctness: PASS\n",
            "\n",
            "--- Testing Optimized Kernel (float3) for 1024x1024 image ---\n",
            "Execution time: 0.0693 ms (averaged over 100 runs)\n",
            "Correctness: PASS\n",
            "\n",
            "--- Testing Optimized Kernel (Shared Memory) for 1024x1024 image ---\n",
            "Execution time: 0.0716 ms (averaged over 100 runs)\n",
            "Correctness: PASS\n",
            "\n",
            "======================================================\n",
            "Processing Image Size: 2048x2048\n",
            "======================================================\n",
            "\n",
            "--- Testing Reference Kernel for 2048x2048 image ---\n",
            "Execution time: 0.2639 ms (averaged over 100 runs)\n",
            "Correctness: PASS\n",
            "\n",
            "--- Testing Optimized Kernel (float3) for 2048x2048 image ---\n",
            "Execution time: 0.2636 ms (averaged over 100 runs)\n",
            "Correctness: PASS\n",
            "\n",
            "--- Testing Optimized Kernel (Shared Memory) for 2048x2048 image ---\n",
            "Execution time: 0.2683 ms (averaged over 100 runs)\n",
            "Correctness: PASS\n",
            "\n",
            "\n",
            "--- Performance Summary (ms) ---\n",
            "-------------------------------------------------------------------------------\n",
            "| Kernel Name                    |   512x512    |  1024x1024   |  2048x2048   |\n",
            "-------------------------------------------------------------------------------\n",
            "| Reference Kernel               |     0.0185ms |     0.0694ms |     0.2639ms |\n",
            "| Optimized Kernel (float3)      |     0.0186ms |     0.0693ms |     0.2636ms |\n",
            "| Optimized Kernel (Shared Memory) |     0.0208ms |     0.0716ms |     0.2683ms |\n",
            "-------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def test_kernel(kernel_func, kernel_name, width, height, input_rgb_np):\n",
        "    \"\"\"Tests a given CUDA kernel for correctness and performance.\"\"\"\n",
        "    print(f\"\\n--- Testing {kernel_name} for {width}x{height} image ---\")\n",
        "\n",
        "    # Prepare host output array\n",
        "    output_gray_np = np.zeros((height, width), dtype=np.float32)\n",
        "\n",
        "    # Allocate GPU memory\n",
        "    input_gpu = cuda.mem_alloc(input_rgb_np.nbytes)\n",
        "    output_gpu = cuda.mem_alloc(output_gray_np.nbytes)\n",
        "\n",
        "    # Copy input data from host (CPU) to device (GPU)\n",
        "    cuda.memcpy_htod(input_gpu, input_rgb_np)\n",
        "\n",
        "    # Define CUDA kernel launch parameters (block and grid dimensions)\n",
        "    if kernel_name == \"Optimized Kernel (Shared Memory)\":\n",
        "        block_dim_x, block_dim_y = 16, 16 # Must match #define TILE_DIM_X/Y\n",
        "    else:\n",
        "        block_dim_x, block_dim_y = 16, 16 # Common default, 256 threads/block\n",
        "\n",
        "    block_dims = (block_dim_x, block_dim_y, 1)\n",
        "    grid_dims = ((width + block_dims[0] - 1) // block_dims[0],\n",
        "                 (height + block_dims[1] - 1) // block_dims[1],\n",
        "                 1)\n",
        "\n",
        "    # Warm-up run (important for stable performance measurements)\n",
        "    kernel_func(input_gpu, output_gpu, np.int32(width), np.int32(height),\n",
        "                block=block_dims, grid=grid_dims)\n",
        "    cuda.Context.synchronize() # Ensure warm-up is complete\n",
        "\n",
        "    # Measure execution time using CUDA events for precision\n",
        "    num_runs = 100 # Average over multiple runs for stability\n",
        "    start_event = cuda.Event()\n",
        "    end_event = cuda.Event()\n",
        "\n",
        "    start_event.record() # Record start time\n",
        "    for _ in range(num_runs):\n",
        "        kernel_func(input_gpu, output_gpu, np.int32(width), np.int32(height),\n",
        "                    block=block_dims, grid=grid_dims)\n",
        "    end_event.record() # Record end time\n",
        "\n",
        "    end_event.synchronize() # Wait for all kernel executions to complete\n",
        "\n",
        "    elapsed_ms_total = start_event.time_till(end_event) # Total time for num_runs in ms\n",
        "    elapsed_ms_per_run = elapsed_ms_total / num_runs\n",
        "\n",
        "    print(f\"Execution time: {elapsed_ms_per_run:.4f} ms (averaged over {num_runs} runs)\")\n",
        "\n",
        "    # Copy output data from device (GPU) to host (CPU)\n",
        "    cuda.memcpy_dtoh(output_gray_np, output_gpu)\n",
        "\n",
        "    # Verify correctness against NumPy calculation\n",
        "    ref_gray_np = (0.2989 * input_rgb_np[:, :, 0] +\n",
        "                   0.5870 * input_rgb_np[:, :, 1] +\n",
        "                   0.1140 * input_rgb_np[:, :, 2])\n",
        "\n",
        "    try:\n",
        "        np.testing.assert_almost_equal(output_gray_np, ref_gray_np, decimal=4)\n",
        "        print(\"Correctness: PASS\")\n",
        "    except AssertionError as e:\n",
        "        print(\"Correctness: FAIL\")\n",
        "        print(e)\n",
        "\n",
        "    # Free GPU memory\n",
        "    input_gpu.free()\n",
        "    output_gpu.free()\n",
        "\n",
        "    return elapsed_ms_per_run\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define image sizes to test (must be square and have even dimensions)\n",
        "    image_sizes = [(512, 512), (1024, 1024), (2048, 2048)]\n",
        "\n",
        "    results_summary = {}\n",
        "\n",
        "    for width, height in image_sizes:\n",
        "        if not (width % 2 == 0 and height % 2 == 0 and width == height):\n",
        "            print(f\"Skipping size {width}x{height} as it does not meet constraints.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n======================================================\")\n",
        "        print(f\"Processing Image Size: {width}x{height}\")\n",
        "        print(f\"======================================================\")\n",
        "\n",
        "        input_rgb_np = np.random.rand(height, width, 3).astype(np.float32)\n",
        "\n",
        "        kernels_to_test = [\n",
        "            (kernel_ref, \"Reference Kernel\"),\n",
        "            (kernel_float3, \"Optimized Kernel (float3)\"),\n",
        "            (kernel_shared_mem, \"Optimized Kernel (Shared Memory)\")\n",
        "        ]\n",
        "\n",
        "        size_str = f\"{width}x{height}\"\n",
        "        for kernel_func, kernel_name in kernels_to_test:\n",
        "            if kernel_name not in results_summary:\n",
        "                results_summary[kernel_name] = {}\n",
        "\n",
        "            exec_time = test_kernel(kernel_func, kernel_name, width, height, input_rgb_np)\n",
        "            results_summary[kernel_name][size_str] = exec_time\n",
        "\n",
        "    # Print summary table\n",
        "    print(\"\\n\\n--- Performance Summary (ms) ---\")\n",
        "    header_cols = [f\"{w}x{h}\" for w,h in image_sizes if w%2==0 and h%2==0 and w==h]\n",
        "    header = f\"| {'Kernel Name':<30} |\" + \"\".join([f\" {col.center(12)} |\" for col in header_cols])\n",
        "    separator = \"-\" * len(header)\n",
        "\n",
        "    print(separator)\n",
        "    print(header)\n",
        "    print(separator)\n",
        "\n",
        "    # Ensure a consistent order for printing results\n",
        "    kernel_order = [\"Reference Kernel\", \"Optimized Kernel (float3)\", \"Optimized Kernel (Shared Memory)\"]\n",
        "    for kernel_name_key in kernel_order:\n",
        "        if kernel_name_key in results_summary:\n",
        "            row = f\"| {kernel_name_key:<30} |\"\n",
        "            for size_key in header_cols:\n",
        "                time_val = results_summary[kernel_name_key].get(size_key, \"N/A\")\n",
        "                if isinstance(time_val, float):\n",
        "                    row += f\" {time_val:>10.4f}ms |\"\n",
        "                else:\n",
        "                    row += f\" {'N/A'.center(12)} |\"\n",
        "            print(row)\n",
        "    print(separator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqdQEPvbUSvi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
